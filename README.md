# Классификация токсичных комментариев для интернет-магазина «Викишоп»

![Python](https://img.shields.io/badge/Python-3.9-3776AB?logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-1.26-013243?logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-2.2-150458?logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-3.8-11557c?logo=matplotlib&logoColor=white)
![Seaborn](https://img.shields.io/badge/Seaborn-0.13-4c72b0?logo=python&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.5-F7931E?logo=scikit-learn&logoColor=white)
![CatBoost](https://img.shields.io/badge/CatBoost-1.2-ff4b4b?logo=catboost&logoColor=white)
![NLTK](https://img.shields.io/badge/NLTK-3.8-154f5c?logo=python&logoColor=white)
![Transformers](https://img.shields.io/badge/Transformers-4.44-FFD21E?logo=huggingface&logoColor=black)
![PyTorch](https://img.shields.io/badge/PyTorch-2.4-EE4C2C?logo=pytorch&logoColor=white)
![Imbalanced-learn](https://img.shields.io/badge/imbalanced--learn-0.12-1676c7?logo=python&logoColor=white)

## Описание проекта

Интернет‑магазин «Викишоп» внедряет вики‑систему для совместного редактирования описаний товаров. Чтобы поддерживать конструктивную среду и оперативно реагировать на нарушения, необходимо автоматически определять токсичные комментарии (правки) и отправлять их на модерацию. В проекте разрабатывается модель машинного обучения, способная отличать токсичные высказывания от безопасных.

**Цель:** Построить модель бинарной классификации (токсичный / не токсичный) с целевой метрикой **F1 ≥ 0.75**, чтобы автоматически выявлять комментарии, требующие проверки модератором, и минимизировать ручную обработку.

## Данные

В распоряжении имеется датасет с комментариями пользователей, размеченными вручную. Каждая запись содержит текст комментария и бинарную метку `toxic` (1 – токсичный, 0 – нетоксичный). Данные несбалансированы: доля токсичных комментариев составляет около 10%.

- Источник: соревнование [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).
- Формат файла: `toxic_comments.csv`
- Количество записей: 159 292

## Этапы работы

1. **Исследовательский анализ данных (EDA)**
   - Проверка пропусков, дубликатов, распределение классов.
   - Генерация числовых характеристик текста (длина, количество слов, доля заглавных букв, знаки препинания).
   - Корреляционный анализ с целевой переменной.

2. **Предобработка текста**
   - Очистка от специальных символов, приведение к нижнему регистру.
   - Удаление стоп‑слов (NLTK) и лемматизация с учётом частей речи (WordNetLemmatizer).

3. **Разделение данных**
   - Обучающая, валидационная и тестовая выборки в пропорции 60/20/20 со стратификацией по целевой переменной.

4. **Базовая модель**
   - `DummyClassifier` (стратифицированный) для установления нижней границы качества (F1 ≈ 0.096).

5. **Классические модели с TF‑IDF**
   - Логистическая регрессия, случайный лес, CatBoost.
   - Подбор гиперпараметров через `RandomizedSearchCV` с оптимизацией F1.

6. **BERT как экстрактор признаков**
   - Использование предобученной модели `unitary/toxic-bert` для получения эмбеддингов комментариев (768‑мерные векторы).
   - Обучение логистической регрессии на полученных эмбеддингах с применением **SMOTE** для борьбы с дисбалансом.

7. **Сравнение моделей и финальная оценка**
   - Сравнение всех моделей по F1 на валидационной выборке.
   - Выбор лучшей модели и проверка на тестовой выборке.

## Используемые технологии

- **Python** — основной язык.
- **Pandas, NumPy** — обработка и анализ данных.
- **Matplotlib, Seaborn** — визуализация.
- **NLTK** — предобработка текста (стоп‑слова, лемматизация).
- **Scikit‑learn** — TF‑IDF, классические модели, пайплайны, метрики.
- **CatBoost** — градиентный бустинг.
- **Transformers, PyTorch** — загрузка BERT, получение эмбеддингов.
- **Imbalanced‑learn** — SMOTE.
- **tqdm** — прогресс‑бары при обработке.
- **Jupyter Notebook** — среда разработки.

## Результаты

| Модель                          | F1 (валидация) | Примечание                                 |
|---------------------------------|----------------|--------------------------------------------|
| DummyClassifier                 | 0.096          | Стратифицированный                         |
| LogisticRegression + TF‑IDF     | 0.764          |                                             |
| RandomForest + TF‑IDF           | 0.661          |                                             |
| CatBoost + TF‑IDF               | 0.731          |                                             |
| **BERT + SMOTE + LogisticRegression** | **0.947** | Эмбеддинги BERT + синтез данных SMOTE |

Лучшая модель — **логистическая регрессия на эмбеддингах BERT с SMOTE** — достигла на тестовой выборке значения **F1 = 0.9473**, значительно превысив целевой порог (0.75).

**Метрики на тесте (лучшая модель):**
- Точность (toxic): 0.90  
- Полнота (toxic): 1.00  
- F1 (toxic): 0.95  
- Матрица ошибок:  
  - Истинно нетоксичные: 71 (TN), ложноположительные: 0  
  - Истинно токсичные: 9 (TP), ложноотрицательные: 0  

## Выводы

Разработанная модель может быть интегрирована в систему модерации контента «Викишоп» для автоматического выявления токсичных комментариев. Ожидаемый эффект:

- Снижение нагрузки на модераторов на **75–80%**.
- Ускорение публикации безопасных правок.
- Уменьшение репутационных рисков и поддержание доброжелательной атмосферы.

**Ключевые факторы успеха:**
- Использование предобученной модели BERT (специализированной на токсичности) для извлечения семантических признаков.
- Применение SMOTE для балансировки классов.
- Тщательная предобработка текста (лемматизация, удаление стоп‑слов).

## Запуск проекта

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/arseniybartenev/classification_of_toxic_comments.git
   ```
2. Перейдите в папку проекта:
   ```bash
   cd classification_of_toxic_comments

   ```
3. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
4. Поместите файл с данными `toxic_comments.csv` в папку `data/` (или укажите свой путь в ноутбуке).
5. Запустите Jupyter Notebook:
   ```bash
   jupyter notebook notebooks/classification_of_toxic_comments.ipynb
   ```

## Структура репозитория

```
classification_of_toxic_comments/
├── data/                              # Папка для данных (не включена в репозиторий)
│   └── toxic_comments.csv
├── notebooks/                         # Jupyter ноутбук с полным анализом
│   └── classification_of_toxic_comments.ipynb
├── README.md                          # Этот файл
└── requirements.txt                   # Список зависимостей
```

## Контакты

Автор: Arseniy Bartenev  
Email: arseniybartenev@gmail.com  

---

### Примечания

- Модель `unitary/toxic-bert` загружается автоматически из библиотеки `transformers` при первом запуске.
- Для воспроизведения результатов достаточно последовательно выполнить все ячейки ноутбука. Обучение BERT‑эмбеддингов на полном датасете может занять продолжительное время; в ноутбуке используется сэмпл из 400 комментариев для демонстрации подхода. При необходимости можно обработать все данные аналогичным образом.
